# EvaluationPipeline ä½¿ç”¨æŒ‡å—

**æ–°åŠŸèƒ½**: è‡ªåŠ¨ç»˜å›¾ + ç‰ˆæœ¬ç®¡ç†å·²é›†æˆåˆ°è¯„ä¼°ç®¡é“

---

## ğŸ¯ æ ¸å¿ƒæ”¹åŠ¨

| åŠŸèƒ½ | è¯´æ˜ |
|------|------|
| **è‡ªåŠ¨ç»˜å›¾** | `save_results()` æ—¶è‡ªåŠ¨ç”Ÿæˆå›¾è¡¨ |
| **ç‰ˆæœ¬ç®¡ç†** | é€šè¿‡ `experiment_name` + `experiment_version` ç®¡ç† |
| **ç»Ÿä¸€è¾“å‡º** | CSV + JSON + å›¾è¡¨ä¸€é”®ç”Ÿæˆ |

---

## ğŸ“Š ä½¿ç”¨æ–¹å¼

### æ–¹å¼ 1: ä½¿ç”¨ EvaluationPipeline ç±»

```python
from src.evaluation import EvaluationPipeline, EvaluationConfig

# é…ç½®
config = EvaluationConfig(
    k_values=[1, 3, 5, 10],
    plot_enabled=True,           # å¯ç”¨è‡ªåŠ¨ç»˜å›¾
    plot_include_performance=True,
)

# åˆ›å»ºç®¡é“ï¼ˆå¸¦ç‰ˆæœ¬ç®¡ç†ï¼‰
pipeline = EvaluationPipeline(
    config=config,
    experiment_name="baseline",   # å®éªŒåç§°
    experiment_version="v1",      # ç‰ˆæœ¬å·
)

# è¯„ä¼°
pipeline.evaluate_batch(eval_data)

# ä¿å­˜ï¼ˆè‡ªåŠ¨ç”Ÿæˆ CSV + JSON + å›¾è¡¨ï¼‰
output_files = pipeline.save_results()
# è¿”å›: {"csv": "...", "summary": "...", "plots_dir": "..."}

# æ‰“å°æ‘˜è¦
pipeline.print_summary()
```

---

### æ–¹å¼ 2: ä½¿ç”¨ä¾¿æ·å‡½æ•°

```python
from src.evaluation import evaluate_rag_results, quick_evaluate

# å®Œæ•´è¯„ä¼°
summary = evaluate_rag_results(
    results=eval_data,
    experiment_name="my_test",
    experiment_version="v2",
    plot_enabled=True,
)

# å¿«é€Ÿè¯„ä¼°
pipeline = quick_evaluate(
    results=eval_data,
    name="quick_test",
    version="v1",
)
```

---

### æ–¹å¼ 3: ä½¿ç”¨è„šæœ¬

```bash
# Vanilla RAG æµ‹è¯•ï¼ˆè‡ªåŠ¨ç‰ˆæœ¬ç®¡ç†å’Œç»˜å›¾ï¼‰
python scripts/run_vanilla_rag.py \
  --mode test \
  --experiment-name "baseline"

# API è¯„ä¼°è„šæœ¬
python scripts/run_evaluation.py \
  --mode test \
  --experiment-name "api_test" \
  --experiment-version "v1"

# ç¦ç”¨ç»˜å›¾
python scripts/run_evaluation.py \
  --mode test \
  --no-plots
```

---

## ğŸ“ è¾“å‡ºæ–‡ä»¶ç»“æ„

```
results/
â”œâ”€â”€ evaluation/                         # CSV æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ v1_baseline.csv                # è¯¦ç»†ç»“æœ
â”‚   â””â”€â”€ v1_baseline_summary.json       # æ‘˜è¦
â”‚
â””â”€â”€ plots/                             # å›¾è¡¨ç›®å½•
    â””â”€â”€ v1_baseline/                   # æŒ‰å®éªŒ ID ç»„ç»‡
        â”œâ”€â”€ v1_baseline_hits.png
        â”œâ”€â”€ v1_baseline_mrr.png
        â”œâ”€â”€ v1_baseline_recall.png
        â”œâ”€â”€ v1_baseline_hit_by_category.png
        â”œâ”€â”€ v1_baseline_latency_distribution.png
        â”œâ”€â”€ v1_baseline_latency_boxplot.png
        â””â”€â”€ v1_baseline_answer_quality.png
```

---

## ğŸ·ï¸ ç‰ˆæœ¬å‘½åè§„åˆ™

**å®Œæ•´å®éªŒ ID** = `{experiment_version}_{experiment_name}`

| experiment_version | experiment_name | å®Œæ•´ ID |
|-------------------|-----------------|---------|
| `v1` | `baseline` | `v1_baseline` |
| `v2` | `with_bm25` | `v2_with_bm25` |
| `None` | `quick_test` | `quick_test` |

---

## ğŸ“Š è‡ªåŠ¨ç”Ÿæˆçš„å›¾è¡¨ï¼ˆ7 å¼ ï¼‰

| # | å›¾è¡¨ | æ–‡ä»¶ååç¼€ |
|---|------|-----------|
| 1 | å‘½ä¸­ç‡åˆ†å¸ƒ | `_hits.png` |
| 2 | MRR åˆ†å¸ƒ | `_mrr.png` |
| 3 | å¬å›ç‡åˆ†å¸ƒ | `_recall.png` |
| 4 | åˆ†ç±»å‘½ä¸­ç‡ | `_hit_by_category.png` |
| 5 | å»¶è¿Ÿåˆ†å¸ƒ | `_latency_distribution.png` |
| 6 | å»¶è¿Ÿç®±çº¿å›¾ | `_latency_boxplot.png` |
| 7 | ç­”æ¡ˆè´¨é‡ | `_answer_quality.png` |

---

## âš™ï¸ é…ç½®é€‰é¡¹

```python
EvaluationConfig(
    # æ£€ç´¢æŒ‡æ ‡
    k_values=[1, 3, 5, 10, 20],
    
    # RAGAS
    ragas_enabled=True,
    
    # WandB
    wandb_enabled=False,
    wandb_project="cuad-assistant",
    
    # è¾“å‡º
    output_dir="results/evaluation",
    
    # ç»˜å›¾
    plot_enabled=True,                    # æ˜¯å¦ç”Ÿæˆå›¾è¡¨
    plot_dir="results/plots",             # å›¾è¡¨ç›®å½•
    plot_include_performance=True,        # å»¶è¿Ÿå›¾
    plot_include_quality=True,            # ç­”æ¡ˆè´¨é‡å›¾
    plot_include_correlation=False,       # ç›¸å…³æ€§çƒ­å›¾
)
```

---

## ğŸ”„ å¤šç‰ˆæœ¬å¯¹æ¯”å®éªŒç¤ºä¾‹

```bash
# ç‰ˆæœ¬ 1: Vanilla
python scripts/run_vanilla_rag.py \
  --mode test \
  --experiment-name "vanilla" \
  --max-samples 100

# ç‰ˆæœ¬ 2: æ·»åŠ  BM25
python scripts/run_evaluation.py \
  --mode test \
  --experiment-name "with_bm25" \
  --experiment-version "v2"

# ç‰ˆæœ¬ 3: æ·»åŠ  Reranker
python scripts/run_evaluation.py \
  --mode test \
  --experiment-name "with_rerank" \
  --experiment-version "v3"
```

**ç»“æœç›®å½•**:
```
results/plots/
â”œâ”€â”€ vanilla_vanilla/
â”œâ”€â”€ v2_with_bm25/
â””â”€â”€ v3_with_rerank/
```

---

## ğŸ“‹ æ§åˆ¶å°è¾“å‡ºç¤ºä¾‹

```
Experiment ID: v1_baseline

Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [01:30<00:00, 1.1it/s]

==================================================
EVALUATION SUMMARY
==================================================

--- Retrieval Metrics ---
  @ 1: Hit=0.1230, MRR=0.1230, Recall=0.1450
  @ 5: Hit=0.3650, MRR=0.2340, Recall=0.4210
  @10: Hit=0.4520, MRR=0.2890, Recall=0.5210

--- Answer Metrics ---
  f1          : 0.3120
  exact_match : 0.0850

--- Latency Metrics ---

  RETRIEVAL:
    Mean:     125.3 ms
    P50:      118.0 ms
    P90:      175.0 ms

  GENERATION:
    Mean:    1850.2 ms
    P50:    1720.0 ms
    P90:    2450.0 ms

==================================================

--------------------------------------------------
OUTPUT FILES
--------------------------------------------------
  csv         : results/evaluation/v1_baseline.csv
  summary     : results/evaluation/v1_baseline_summary.json
  plots_dir   : results/plots/v1_baseline
--------------------------------------------------
```

---

## ğŸ‰ å¿«é€Ÿå¼€å§‹

```bash
# æœ€ç®€ç”¨æ³•
python scripts/run_vanilla_rag.py --mode test --max-samples 10

# æŸ¥çœ‹è¾“å‡º
ls results/evaluation/
ls results/plots/
```

**æ‰€æœ‰è¯„ä¼°ç»“æœå’Œå›¾è¡¨å°†è‡ªåŠ¨ç”Ÿæˆå’Œç‰ˆæœ¬åŒ–ï¼** ğŸ“Šâœ¨
