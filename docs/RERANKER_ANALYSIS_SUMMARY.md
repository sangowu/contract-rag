# Reranker模型显存和性能问题分析总结

## 问题1：为什么reranker模型会占用23G显存，并且耗时很长？

### 显存占用23G的原因

#### 1. 激活值（Activations）显存占用（主要原因）

虽然模型权重使用4bit量化（约5G），但**推理时的激活值仍然是float16/bfloat16**，这是显存占用的主要来源。

**计算公式**：
```
激活值显存 ≈ batch_size × seq_len × hidden_size × num_layers × 2 (float16) × 中间层缓存倍数
```

**对于Qwen3-Reranker-4B模型**：
- hidden_size ≈ 4096
- num_layers ≈ 32
- **原始配置**：batch_size=32, max_length=1024
  - 激活值显存 ≈ 32 × 1024 × 4096 × 32 × 2 × 2 ≈ **16-20GB**
- **当前配置**：batch_size=64, max_length=256
  - 激活值显存 ≈ 64 × 256 × 4096 × 32 × 2 × 2 ≈ **8-12GB**

#### 2. 模型权重显存（4bit量化后）

- 4bit量化后模型权重：约5G
- 量化参数（scale/zero_point）：约1-2G
- **总计：6-7GB**

#### 3. KV Cache和其他开销

- Attention机制的KV cache
- 中间计算结果缓存
- PyTorch框架开销
- **总计：2-4GB**

#### 4. 显存占用总计

**原始配置（batch_size=32, max_length=1024）**：
- 权重：6-7GB
- 激活值：16-20GB
- 其他：2-4GB
- **总计：24-31GB**（超过23GB，导致OOM）

**当前配置（batch_size=64, max_length=256）**：
- 权重：6-7GB
- 激活值：8-12GB
- 其他：2-4GB
- **总计：16-23GB**（仍然接近上限）

### 耗时长的原因

1. **批处理大小过大**：
   - batch_size=64对于4B参数的模型来说太大
   - 导致单批处理时间长
   - 可能触发显存交换，进一步拖慢速度

2. **缺少优化**：
   - 使用`torch.no_grad()`而非`torch.inference_mode()`
   - 数据传输未使用异步（non_blocking）
   - 未使用`torch.compile()`进行图优化

3. **内存管理不当**：
   - GPU缓存清理不够及时
   - 可能存在显存碎片化

4. **序列长度**：
   - max_length=256仍然较长（虽然已从1024降低）

## 问题2：如何优化处理速度？

### 已实施的优化

1. **减小批处理大小**：
   - 从64降低到8
   - 显存占用减少约87.5%
   - 单批处理速度提升

2. **减少序列长度**：
   - 从1024降低到256
   - 激活值显存减少约75%

3. **使用torch.inference_mode()**：
   - 替代`torch.no_grad()`
   - 禁用更多梯度相关计算，提升速度

4. **异步数据传输**：
   - 使用`non_blocking=True`
   - 可以重叠计算和数据传输

5. **定期清理GPU缓存**：
   - 每处理4个batch后清理一次
   - 避免显存碎片化

### 优化后的预期效果

**显存占用**：
- 权重：6-7GB
- 激活值（batch_size=8, max_length=256）：约1-2GB
- 其他：1-2GB
- **总计：8-11GB**（相比23GB减少约50-65%）

**处理速度**：
- 单批处理速度提升（batch_size更小）
- 总时间可能略增（批次数增加），但更稳定
- 避免OOM错误，提高系统稳定性

### 进一步优化建议（可选）

1. **使用torch.compile()**（PyTorch 2.0+）：
   - 可以加速20-30%
   - 但需要额外编译时间

2. **动态批处理大小**：
   - 根据剩余显存动态调整batch_size
   - 最大化利用显存

3. **使用更小的模型**：
   - 如果精度可接受，考虑使用更小的reranker模型

## 问题3：reranker模型在4bit量化之后加载大约在5G显存左右，还有什么使用了GPU1的显存导致在stage 1阶段就占满了显存？

### GPU1显存占用详细分析

#### 1. Embedding模型（all-MiniLM-L6-v2）

**位置**：`src/rag/embedding.py`的`get_model()`函数

**显存占用**：
- 模型权重：约80MB
- 推理时激活值：约50-100MB
- **总计：约150-200MB**

**说明**：虽然模型较小，但在GPU1上与Reranker模型共享显存。

#### 2. Reranker模型（Qwen3-Reranker-4B，4bit量化）

**位置**：`src/rag/retrieval.py`的`get_reranker_model()`函数

**显存占用**：
- 模型权重（4bit量化）：约5G
- 量化参数和缓存：约1-2G
- **总计：约6-7GB**

**说明**：这是GPU1上的主要显存占用。

#### 3. Reranker推理时的激活值（关键！）

**这是导致显存占满的主要原因！**

**激活值显存占用**：
- **batch_size=64, max_length=256时**：约8-12GB
- **batch_size=8, max_length=256时**：约1-2GB

**说明**：
- 激活值显存占用与batch_size和max_length成正比
- 即使模型权重只有5G，激活值可能占用10G+
- 这是导致23G显存占满的主要原因

#### 4. 显存碎片化

**原因**：
- PyTorch内存分配器可能产生碎片
- 多次分配和释放导致碎片化

**占用**：约1-2GB

#### 5. Stage 1阶段显存占用总计

**原始配置（batch_size=64, max_length=256）**：
- Embedding模型：0.2GB
- Reranker权重：6-7GB
- Reranker激活值：8-12GB
- 碎片化：1-2GB
- **总计：15-21GB**（接近23GB上限）

**优化后配置（batch_size=8, max_length=256）**：
- Embedding模型：0.2GB
- Reranker权重：6-7GB
- Reranker激活值：1-2GB
- 碎片化：0.5-1GB
- **总计：7.7-10.2GB**（安全范围）

### 关键发现

**激活值显存占用是主要问题！**

即使模型权重只有5G（4bit量化后），但推理时的激活值可能占用10G+，这是导致显存占满的主要原因。

**解决方案**：
1. 减小batch_size（最重要）
2. 减小max_length
3. 定期清理GPU缓存
4. 使用更高效的推理模式

## 总结

### 问题根源

1. **显存占用23G**：主要是激活值占用（8-12GB），而非模型权重（6-7GB）
2. **耗时长**：batch_size过大，缺少优化
3. **Stage 1占满显存**：激活值占用是主要原因，而非模型权重

### 优化措施

1. ✅ 减小batch_size：64 → 8
2. ✅ 减小max_length：1024 → 256
3. ✅ 使用torch.inference_mode()
4. ✅ 使用non_blocking传输
5. ✅ 定期清理GPU缓存

### 预期效果

- 显存占用：23GB → 8-11GB（减少50-65%）
- 处理速度：单批更快，总时间可能略增但更稳定
- 系统稳定性：避免OOM错误
